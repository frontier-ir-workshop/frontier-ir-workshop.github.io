,,,,,,,
AAAI26_W41_1,Annotating Pretraining Corpora in Information Extraction: An Unsupervised Confidence-Guided Approach,"Many recent zero or few-shot approaches in Information Extraction rely on large-scale annotated corpora for pretraining. However, while manually annotated corpora are scarce, the current methods for performing data augmentation or data generation remain limited, especially for complex tasks such as event extraction. To address this challenge, we introduce a simple yet efficient annotation method for real-world data that leverages the capabilities of Large Language Models (LLMs) to both generate annotations and evaluate their quality. Our method defines confidence scores to identify passages of interest and to select the best generated annotations. Based on this approach, we propose Omnivent, a large-scale and general-purpose event dataset designed for both event detection and event argument extraction. To the best of our knowledge, our dataset offers the largest coverage of event types to date, comprising 38,859 distinct event types and 9,981 argument roles. To demonstrate the effectiveness of our dataset, we also introduce GLEE, a bidirectional transformer model to perform zero-shot event detection and argument extraction. We conducted evaluations across 9 benchmarks and 7 domains showing that pretraining on Omnivent yields better zero-shot performance than pretraining on human-annotated data. Notably, despite being 25x smaller, GLEE outperforms leading LLM-based approaches in zero-shot event detection and achieves up to 50\% of the performance of fully supervised models in event argument extraction.","Salim Abdou Daoura, Olivier Ferret, Romaric Besançon, Sondes Souihi","salim.abdoudaoura@cea.fr, olivier.ferret@cea.fr, romaric.besancon@cea.fr, sondes.souihi@cea.fr",,,
AAAI26_W41_2,FIRE-RAG: Fused Iterative Retrieval Enhancement for Robust Multimodal Generation,"Multimodal Retrieval-Augmented Generation (RAG) enhances Multimodal Large Language Models (MLLMs) by grounding generation in external evidence. However, existing frameworks typically rely on static, one-shot retrieval pipelines with shallow integration techniques, such as feature concatenation. This paradigm struggles with complex, multi-hop queries where evidence must be gathered sequentially. While iterative retrieval offers a theoretical solution, it introduces a critical challenge: contextual noise accumulation, where irrelevant documents retrieved in later steps dilute the model's focus. To address this, we introduce FIRE-RAG (Fused Iterative Retrieval Enhancement), a framework that reframes generation as a dynamic, agentic process. Unlike prior methods, FIRE-RAG couples an Adaptive Retrieval strategy—which allows the model to actively decide when to retrieve—with a Transformer-based Interaction Fuser. This fuser employs joint self-attention across the query, visual inputs, and the full history of retrieved documents, enabling the model to synthesize coherent evidence chains while suppressing noise. Extensive evaluations on the E-VQA and InfoSeek benchmarks demonstrate that FIRE-RAG achieves competitive performance, showing particular strength in complex reasoning tasks compared to static baselines. Our analysis further validates that deep fusion is a prerequisite for effective iterative retrieval in multimodal settings.","Zhaoyang Ye, Bozhen Ren, Li C Xia","zhaoyang.scott.ye@gmail.com, mabren@mail.scut.edu.cn, lcx.scut@outlook.com",,,
AAAI26_W41_3,Eigen-1: Adaptive Multi-Agent Refinement with Monitor-Based RAG for Scientific Reasoning,"Large language models (LLMs) have recently shown strong progress on scientific reasoning, yet two major bottlenecks remain. First, explicit retrieval fragments reasoning, imposing a hidden ``tool tax'' of extra tokens and steps. Second, multi-agent pipelines often dilute strong solutions by averaging across all candidates. We address these challenges with a unified framework that combines implicit retrieval and structured collaboration. At its foundation, a \emph{Monitor-based retrieval module} operates at the token level, integrating external knowledge with minimal disruption to reasoning. On top of this substrate, \emph{Hierarchical Solution Refinement (HSR)} iteratively designates each candidate as an anchor to be repaired by its peers, while \emph{Quality-Aware Iterative Reasoning (QAIR)} adapts refinement to solution quality. On Humanity’s Last Exam (HLE) Bio/Chem Gold, our framework achieves 48.3\% accuracy---the highest reported to date, surpassing the strongest agent baseline by 13.4 points and leading frontier LLMs by up to 18.1 points, while simultaneously reducing token usage by 53.5\% and agent steps by 43.7\%. Results on SuperGPQA and TRQA confirm robustness across domains. Error analysis shows that reasoning failures and knowledge gaps co-occur in over 85\% of cases, while diversity analysis reveals a clear dichotomy: retrieval tasks benefit from solution variety, whereas reasoning tasks favor consensus. Together, these findings demonstrate how implicit augmentation and structured refinement overcome the inefficiencies of explicit tool use and uniform aggregation.","Xiangru Tang, Wanghan Xu, Yujie Wang, Zijie Guo, Daniel Shao, Jiapeng Chen, Cixuan Zhang, Ziyi Wang, Lixin Zhang, Guancheng Wan, Wenlong Zhang, LEI BAI, Zhenfei Yin, Philip Torr, Hanrui Wang, Di Jin","xiangru.tang@yale.edu, xu_wanghan@sjtu.edu.cn, yujie.wang.yw2233@yale.edu, zjguo24@m.fudan.edu.cn, yanjun.shao@yale.edu, jiapeng.chen@yale.edu, cixuan_zhang@outlook.com, ziyi__wang@outlook.com, 2150343@tongji.edu.cn, gcwan03@ucla.edu, zhangwenlong@pjlab.org.cn, baisanshi@gmail.com, jeremyyin@robots.ox.ac.uk, philip.torr@eng.ox.ac.uk, hanrui@mit.edu, jindi930617@gmail.com",,,
AAAI26_W41_4,Joint Spherical Distance and Confusing Triplet Embeddings for Explainable Image Retrieval,"Despite the remarkable advancements in recent artificial intelligence technologies, their inherent lack of explainability often renders models opaque to human users, eroding trust and failing to meet the core requirements of human-centric applications. To bridge this gap, we propose Explainable Image Retrieval (EIR) as a novel method grounded in principles of explainability, leveraging deep metric learning. Motivated by the uniform distribution of normalized feature vectors on a unit hypersphere, we first define spherical distance as a new metric to enhance transparency in embedding spaces. Then, leveraging the spherical distance, we investigate the impact of confusing triplet embeddings on the performance of triplet-based metric learning methods, and develop a new metric learning method termed Enhanced Confusing Triplet Embedding Learning (ECTEL), which harnesses the semantics and knowledge in the training data. Combining spherical distance and ECTEL, the proposed EIR demonstrates strong explainability and accuracy, enabling its application to scenarios in human-centric applications.","Yinan Tang, Lingmei Dong, Zhenhua Guo, Li Wang, Hongwei Zhang, Gao Kai, Fang Cao","tangyinan@ieisystem.com, donglingmei@citicbank.com, guozhenhua@inspur.com, wang_libj@ieisystem.com, zhanghongwei07@ieisystem.com, gaokaibj@ieisystem.com, caofang@ieisystem.com",,,
AAAI26_W41_5,M3DR - Towards Universal Multilingual Multimodal Document Retrieval,"Multimodal document retrieval systems have shown strong progress in aligning visual and textual content for semantic search. However, most existing approaches remain heavily English-centric, limiting their effectiveness in multilingual contexts. In this work, we present M3DR (Multilingual Multimodal Document Retrieval) a framework designed to bridge this gap across languages, enabling applicability across diverse linguistic and cultural contexts. M3DR leverages synthetic multilingual document data and generalizes across different vision-language architectures and model sizes, enabling robust cross-lingual and cross-modal alignment. Using contrastive training, our model learns unified representations for text and document images that transfer effectively across languages. We validate this capability on 22 typologically diverse languages, demonstrating consistent performance and adaptability across linguistic and script variations. We further introduce a comprehensive benchmark that captures real-world multilingual scenarios, evaluating models under monolingual, multilingual, and mixed-language settings. Extensive experiments show that M3DR achieves substantial improvements over existing baselines, with over 150\% relative gains on cross-lingual retrieval tasks.","Adithya S Kolavi, Vyoman Jain","adithyaskolavi@gmail.com, pes2ug22cs672@pesu.pes.edu",,,
AAAI26_W41_6,RAGuard: A Layered Defense Framework for Retrieval-Augmented Generation Systems Against Data Poisoning,"Retrieval-Augmented Generation (RAG) systems are becoming more common in augmenting large language models (LLMs) with factual knowledge, yet they remain highly vulnerable to data poisoning, i.e., maliciously injected passages that manipulate retrieved evidence. We introduce RAGuard, a layered two-step defense framework that combines retrieval-level adversarial training with a novel zero-knowledge inference patch. The first step fine-tunes dense retrievers (e.g., Contriever, compatible with BGE and others) using synthetic poisoned documents (composed of poisons such as fabricated facts, contradictions, and reasoning traps), training them to downrank malicious passages. The second step applies a black-box approach zero knowledge inference patch that identifies and filters suspicious documents based on their causal influence on QA correctness, without requiring poison labels. Experiments on Natural Questions (NQ) and Benchmarking-IR (BEIR) show that RAGuard improves robustness by reducing the Attack Success Rate (ASR) while maintaining retrieval quality (Recall@5, MRR). Together, these layers offer an efficient and label-free defense against both known and unseen poisoning attacks, establishing a general framework for resilient, self-healing RAG pipelines.","Pushkal Kumar, Tanish Kolhe, Shubham Zala, Tucker Nielson, Vincent Li, Michael Saxon, Sean Wu, Kevin Zhu","pushkalku@gmail.com, tanish.kolhe@gmail.com, shubham.zala2009@gmail.com, suzannes@bu.edu, vinli@bu.edu, saxon@ucsb.edu, sean.wu@pepperdine.edu, kevin@algoverseairesearch.org",,,
AAAI26_W41_7,Continual Pre-training of Dense Retrievers for Bridging Paraphrases and Technical Terms in Domain-Specific Retrieval-Augmented Generation,"Retrieval-Augmented Generation (RAG) has facilitated the construction of domain-specific question–answering (Q&A) systems by integrating information retrieval (IR) mechanisms with large language models (LLMs). However, real-world RAG systems frequently suffer from domain drift In automotive voice assistants, novice users unfamiliar with technical terminology often paraphrase such terms, which prevents dense retrievers from bridging the semantic gap and consequently causes retrieval failures. The rapid proliferation of LLMs has expanded their user base, increasing the demand for non-expert users to access domain-specific knowledge in specialized fields such as healthcare and law, thereby underscoring the significance of this issue. This study investigates a practical approach to addressing the challenge of understanding paraphrastic expressions of technical terminology, using a dataset of over 10,000 Q&A pairs derived from vehicle manuals, and proposes continual pre-training of embedding models as an effective solution. Experimental results demonstrate that continual pre-training effectively imbues embedding models with domain knowledge, enabling them to associate paraphrased expressions with their corresponding technical terms. Although continual pre-training has been extensively studied for language models, its application to retrieval embedding models within RAG systems remains largely underexplored. Our findings offer insights into designing retrieval systems that achieve both domain fidelity and linguistic generalization—two essential prerequisites for scaling RAG to real-world applications.","Yusuke Yamaura, Daisuke Kimura, Jiro Nishitoba, Yohei Wakisaka, Shintaro Fukushima","yusuke_yamaura_aa@mail.toyota.co.jp, daisuke.kimura@retrieva.jp, jiro.nishitoba@retrieva.jp, youhei_wakisaka@mail.toyota.co.jp, sfukushim@gmail.com",,,
AAAI26_W41_8,Retrieving Objects from 3D Scenes with Box-Guided Open-Vocabulary Instance Segmentation,"Locating and retrieving objects from scene-level point clouds is a challenging problem with broad applications in robotics and augmented reality. This task is commonly formulated as open-vocabulary 3D instance segmentation. Although recent methods demonstrate strong performance, they depend heavily on SAM and CLIP to generate and classify 3D instance masks from images accompanying the point cloud, leading to substantial computational overhead and slow processing that limit their deployment in real-world settings. Open-YOLO 3D alleviates this issue by using a real-time 2D detector to classify class-agnostic masks produced directly from the point cloud by a pretrained 3D segmenter, eliminating the need for SAM and CLIP and significantly reducing inference time. However, Open-YOLO 3D often fails to generalize to object categories that appear infrequently in the 3D training data. In this paper, we propose a method that generates 3D instance masks for novel objects from RGB images guided by a 2D open-vocabulary detector. Our approach inherits the 2D detector’s ability to recognize novel objects while maintaining efficient classification, enabling fast and accurate retrieval of rare instances from open-ended text queries.","Khanh Nguyen, Dasith de Silva Edirimuni, Ghulam Mubashar Hassan, Ajmal Saeed Mian","duykhanh.nguyen@research.uwa.edu.au, dasith.desilva@uwa.edu.au, ghulam.hassan@uwa.edu.au, ajmal.mian@uwa.edu.au",,,
AAAI26_W41_9,Unified Interactive Multimodal Moment Retrieval via Cascaded Embedding-Reranking and Temporal-Aware Score Fusion,"The exponential growth of video content has created an urgent need for efficient multimodal video retrieval systems. However, existing approaches face three critical challenges: (1) fixed-weight fusion strategies fail under cross-modal noise and ambiguous queries, (2) temporal modeling struggles to capture coherent event sequences while penalizing unrealistic gaps, and (3) systems require manual modality selection, reducing usability. We propose a unified multimodal video retrieval system with three key innovations. First, a cascaded dual-embedding pipeline combines BEiT-3 and SigLIP for broad retrieval, refined by BLIP-2 based reranking to balance recall and precision. Second, a temporal-aware scoring mechanism applies exponential decay penalties to large temporal gaps via beam search, constructing coherent event sequences rather than isolated frames. Third, LLM-guided query decomposition (GPT-4o) automatically interprets ambiguous queries, decomposes them into modality-specific sub-queries (visual/OCR/ASR), and performs adaptive score fusion eliminating manual modality selection. Qualitative analysis demonstrates that our system effectively handles ambiguous queries, retrieves temporally coherent sequences, and dynamically adapts fusion strategies, advancing interactive video search capabilities.","Le Ngo Thanh Toan, Huu Phat Ha, Nguyễn Đặng Duy Tân, Minh Thong Nguyen Le, Tinh-Anh Nguyen-Nhu","23521603@gm.uit.edu.vn, 22521067@gm.uit.edu.vn, nddtan2011@gmail.com, thongnlm29@mp.hcmiu.edu.vn, anh.nguyennhu2306@hcmut.edu.vn",,,
AAAI26_W41_10,GamiBench: Evaluating Spatial Reasoning and 2D-to-3D Planning Capabilities of MLLMs with Origami Folding Tasks,"Multimodal large language models (MLLMs) are proficient in perception and instruction-following, but they still struggle with spatial reasoning: the ability to mentally track and manipulate objects across multiple views and over time. Spatial reasoning is a key component of human intelligence, but most existing benchmarks focus on static images or final outputs, failing to account for the sequential and viewpoint-dependent nature of this skill. To close this gap, we introduce GamiBench, a benchmark designed to evaluate spatial reasoning and 2D-to-3D planning in MLLMs through origami-inspired folding tasks. GamiBench includes 186 regular and 186 impossible 2D crease patterns paired with their corresponding 3D folded shapes, produced from six distinct viewpoints across three visual question-answering (VQA) tasks: predicting 3D fold configurations, distinguishing valid viewpoints, and detecting impossible patterns. Unlike previous benchmarks that assess only final predictions, GamiBench holistically evaluates the entire reasoning process of the models; measuring cross-view consistency, physical feasibility through impossible-fold detection and interpretation of intermediate folding steps. It further introduces new diagnostic metrics—viewpoint consistency (VC) and impossible fold selection rate (IFSR)—to measure how well models handle folds of varying complexity. By linking geometric evaluation with sequential reasoning, GamiBench enables a comprehensive evaluation of state-of-the-art MLLMs, revealing significant limitations in spatial reasoning capabilities, such as multi-view inconsistency and difficulty detecting physically impossible folds. Our experiments show that even leading models such as GPT-5 and Gemini-2.5-Pro struggle on single-step spatial understanding, while other MLLMs tend to show highly variable or inconsistent answering trends. These contributions establish a standardized framework for evaluating and advancing geometric understanding and spatial reasoning in MLLMs. The GamiBench dataset and code will be made available upon publication.","Ryan Spencer, Roey Yaari, Ritvik Vemavarapu, Joyce Yang, Steven Ngo, Utkarsh Sharma","rtspencer.bd@gmail.com, roey_yaari24@yahoo.com, ritvik.vema@gmail.com, pinkeesmur@gmail.com, svngo@ucsd.edu, sharmautkarsh0504@gmail.com",,,
AAAI26_W41_11,CREAM-RAG: Enhanced Retrieval Augmented Generation to Limit Hallucination through Consistency-based Self-RAG,"Retrieval-Augmented Generation (RAG) systems enhance large language models (LLMs) by grounding responses in external evidence, mitigating hallucinations, and enabling access to up-to-date, domain-specific knowledge. However, existing RAG frameworks often suffer from unstable self-supervised optimization signals and inconsistent factual grounding. We introduce CREAM-RAG (Consistency-Regularized Enhanced Augmented Model for RAG), a unified framework that integrates retrieval, Direct Preference Optimization (DPO)-based self-reward reinforcement learning, and a consistency regularization objective to stabilize reward dynamics during fine-tuning. By enforcing alignment between multiple retrieved contexts and generated responses, CREAM-RAG improves factual faithfulness and semantic coherence without external supervision. Empirical evaluations on the LLaMA-2-7B model demonstrate that CREAM-RAG achieves a 35.04% average improvement over the base model across reasoning and factuality benchmarks, highlighting its effectiveness in reducing hallucinations and enhancing retrieval-grounded reasoning.","Yuliah Louis, Vivek Sekhadia, James Vaisman, Kingston Huynh, Sri Yanamandra, Kevin Zhu, Ryan Lagasse","yuliahlouis@gmail.com, vivsekh@gmail.com, vaismanjames@gmail.com, kingston.a.huynh@gmail.com, sriy2@illinois.edu, kevin@algoverseairesearch.org, ryanplagasse@gmail.com",,,
AAAI26_W41_12,LLandMark: A Multi-Agent Framework for Landmark-Aware Multimodal Interactive Video Retrieval,"The increasing diversity and scale of video data demand retrieval systems capable of multimodal understanding, adaptive reasoning, and domain-specific knowledge integration. This paper presents LLandMark, a modular multi-agent framework for landmark-aware multimodal video retrieval to handle real-world complex queries. The framework features specialized agents that collaborate across four stages: query parsing and planning, landmark reasoning, multimodal retrieval, and reranked answer synthesis. A key component, the Landmark Knowledge Agent, detects cultural or spatial landmarks and reformulates them into descriptive visual prompts, enhancing CLIP-based semantic matching for Vietnamese scenes. To expand capabilities, we introduce an LLM-assisted image-to-image pipeline, where a large language model (Gemini 2.5 Flash) autonomously detects landmarks, generates image search queries, retrieves representative images, and performs CLIP-based visual similarity matching, removing the need for manual image input. In addition, an OCR refinement module leveraging Gemini and LlamaIndex improves Vietnamese text recognition. Experimental results show that LLandMark achieves adaptive, culturally grounded, and explainable retrieval performance. Our code will be released soon.","Phung Minh Chi, Le Thien Bao, Tu Tran Thi Cam, Nguyen Thi Thu Dieu, Dao Vu Hung, Tinh-Anh Nguyen-Nhu","23520179@gm.uit.edu.vn, 23520110@gm.uit.edu.vn, 23521704@gm.uit.edu.vn, 23520289@gm.uit.edu.vn, 23520554@gm.uit.edu.vn, anh.nguyennhu2306@hcmut.edu.vn",,,
AAAI26_W41_13,Structured Language Generation Model: Loss Calibration and Formatted Decoding for Robust Structure Prediction,"Modern generative pre-trained language models excel at open-ended text generation, yet continue to underperform on structure-related tasks such as NER, relation extraction, and semantic role labeling, especially when compared to encoder-only models of similar sizes. While this gap has been attributed to limited structure knowledge, we hypothesize this is also due to the missing connection between the model’s internal representations of linguistic structure and the output space used during supervised fine-tuning. We propose the Structured Language Generation Model (SLGM), a model- and task-agnostic framework that reformulates structured prediction as a classification problem through three components: (1) reinforced input formatting with structural cues, (2) loss design, and (3) format-aware decoding that constrains generation to task-valid outputs. Across 5 tasks and 13 datasets, SLGM substantially improves structure prediction without relying on dataset-specific engineering or additional model parameters. It outperforms baseline fine-tuning on models of the same size, achieves comparable performance to much larger models when used with $<$1B parameter models, and acts as a zero-weight adapter that reproduces the benefits of dataset-specific fine-tuning in low-resource settings.","Minho Lee, Junghyun Min, Yerang Kim, Lee Woochul, Yeonsoo Lee","minolee@ncsoft.com, jm3743@georgetown.edu, pangyodevelopercanho@gmail.com, darkgeo@ncsoft.com, yeonsoo@ncsoft.com",,,
AAAI26_W41_14,An Analysis of Hyper-Parameter Optimization Methods for Retrieval Augmented Generation,"Optimizing Retrieval-Augmented Generation (RAG) configurations for specific tasks is a complex and resource-intensive challenge. Motivated by this challenge, frameworks for RAG hyper-parameter optimization (HPO) have recently emerged, yet their effectiveness has not been rigorously benchmarked. To fill this gap, we present a comprehensive study involving five HPO algorithms over five datasets from diverse domains, including a newly curated one on real-world product documentation. Our study explores the largest HPO search space considered to date, with three evaluation metrics as optimization targets. Analysis of the results shows that RAG HPO can be done efficiently, either greedily or with random search, and that it significantly boosts RAG performance for all datasets. For greedy HPO approaches, we show that optimizing model selection first is preferable to the common practice of following the RAG pipeline order during optimization.","Matan Orbach, Ohad Eytan, Benjamin Sznajder, Ariel Gera, Odellia Boni, Yoav Kantor, Gal Bloch, Omri Levy, Hadas Abraham, Nitzan Barzilay, Eyal Shnarch, Michael E. Factor, Shila Ofek-Koifman, Paula Ta-Shma, Assaf Toledo","matan.orbach@gmail.com, ohad.eytan1@ibm.com, benjams@il.ibm.com, ariel.gera1@ibm.com, odelliab@il.ibm.com, yoavka@il.ibm.com, gal.bloch@ibm.com, omrilevy@mail.tau.ac.il, hadasabraham@campus.technion.ac.il, nitzan.barzilay@mail.huji.ac.il, eyals@il.ibm.com, michael_factor@yahoo.com, shila@il.ibm.com, paula@il.ibm.com, assaf.toledo@ibm.com",,,
AAAI26_W41_15,Fisher-LD: Layer-Wise Knowledge Distillation for LLM Recommenders,"We present Fisher-LD, a layer-wise knowledge distillation framework for transformer-based recommenders. Fisher-LD uses the Fisher Information Matrix to quantify per-layer importance, allocating supervision where it matters most. On MovieLens-1M, our 6-layer student achieves HR@10=0.978 with 3.3× compression and 3.2× speedup, exceeding the 12-layer teacher (HR@10=0.934) by 4.4 percentage points. Cross-domain validation on Amazon Reviews confirms generalization. Experiments against five baselines show consistent improvements. Our three-phase protocol—Fisher analysis, selective distillation, task fine-tuning—enables effective compression for production LLM-based recommenders. Code will be released.",Zhaohui Geoffrey Wang,zwang000@usc.edu,,,
AAAI26_W41_16,jina-reranker-v3: Last but Not Late Interaction for Listwise Document Reranking,"jina-reranker-v3 is a 0.6B-parameter multilingual listwise reranker that introduces a novel ""last but not late"" (LBNL) interaction mechanism. Unlike late interaction models like ColBERT that encode documents separately before multi-vector matching, our approach enables cross-document interactions during encoding by processing queries and all candidate documents simultaneously within shared context windows. We extract contextual embeddings from special tokens at each document's end (the ""last"" position), but crucially, interactions occur throughout the encoding process (""not late""). The model achieves state-of-the-art BEIR performance with 61.9 nDCG@10 while being significantly smaller than comparable alternatives.","Feng Wang, Han Xiao","felix.wang@jina.ai, han.xiao@jina.ai",,,
AAAI26_W41_17,DigiMAC-FL: Towards Communication-Aware Federated Optimization for Reasoning-Enhanced Retrieval,"The rapid growth of connected devices has made federated learning (FL) essential for privacy-preserving intelligence at the edge. This, in combination with the growing demand for over-parameterized neural networks, large-scale foundation and reasoning models, creates a fundamental communication and information retrieval bottleneck. Therefore, we aim to provide a solution by alleviating the FL uplink communication bottleneck by proposing \emph{Digital MAC Federated Learning (DigiMAC-FL)}, a novel, fully digital-transmission-based aggregation framework that performs quantized model averaging directly within the multiple access channel. Each client transmits QAM-encoded model increments, enabling simultaneous uplink aggregation and exact arithmetic averaging in the complex baseband. Unlike analog over-the-air schemes, DigiMAC--FL operates within standard digital modulation while preserving the linearity required for theoretical analysis. Experiments on Fashion-MNIST and CIFAR-10 confirm that DigiMAC--FL achieves near--FedAvg accuracy with more than a tenfold reduction in communication cost. The observed Pareto-optimal point at four bits per parameter matches the analytically predicted convergence threshold, demonstrating an exact bit--accuracy trade-off. These results highlight DigiMAC-FL as a practical bridge between wireless communication and distributed reasoning systems, paving the way for scalable, communication-aware retrieval and foundation model training.",Gagandeep Kaur,gagandeepk@iiitd.ac.in,,,
AAAI26_W41_18,ReFeed: Retrieval Feedback–Guided Dataset Construction for Style-Aware Query Rewriting,"Retrieval systems often fail when user queries differ stylistically or semantically from the language used in domain documents. Query rewriting has been proposed to bridge this gap, improving retrieval by reformulating user queries into semantically equivalent forms. However, most existing approaches overlook the stylistic characteristics of target documents—their domain-specific phrasing, tone, and structure—which are crucial for matching real-world data distributions. We introduce a retrieval feedback–driven dataset generation framework that automatically identifies failed retrieval cases, leverages large language models to rewrite queries in the style of relevant documents, and verifies improvement through re-retrieval. The resulting corpus of (original, rewritten) query pairs enables the training of rewriter models that are explicitly aware of document style and retrieval feedback. This work highlights a new direction in data-centric information retrieval, emphasizing how feedback loops and document-style alignment can enhance the reasoning and adaptability of RAG systems in real-world, domain-specific contexts.","Jiyoon Myung, JUNGKI SON, Kyungro lee, Jihyeon Park, Joohyung Han","jiyoon0424@gmail.com, aeolian83@gmail.com, lkr981147@gm.gist.ac.kr, milhaud1201@gmail.com, ddang8jh@gmail.com",,,
AAAI26_W41_19,Retrieval with Multiple Query Vectors through Anomalous Pattern Detection,"A classical vector retrieval problem typically considers a \emph{single} query embedding vector as input and retrieves the most similar embedding vectors from a vector database. However, complex reasoning and retrieval tasks frequently require \emph{multiple query vectors}, rather than a single one. In this work, we propose a retrieval method that considers multiple query vectors simultaneously and retrieves the most relevant vectors from the database using concepts from anomalous pattern detection. Specifically, our approach leverages a set of query vectors $Q$ (with $|Q|\geq 1$), and identifies the subset of vector dimensions within $Q$ that standout (anomalous) from the rest of dimensions. Next, we scan the vector database to retrieve the set of vectors that are also anomalous across the previously identified vector dimensions and return them as our retrieved set of vectors. We validate our approach on two image datasets, a text dataset, and a tabular dataset. Overall, we observe that, across most datasets, larger query sets lead to improved retrieval performance. The improvement is most pronounced when increasing the query sets from 1 to 8, while the gains become smaller beyond that.","ALLASSAN TCHANGMENA A NKEN, BAIMAM BOUKAR JEAN JACQUES, Miriam Rateike, Celia Cintas, Skyler Speakman","a.tchangmenaanken1@universityofgalway.ie, bbaimamb@andrew.cmu.edu, miriam.rateike@ibm.com, celia.cintas@ibm.com, skyler@ke.ibm.com",,,
AAAI26_W41_20,Soft Filtering : Guiding Zero-shot Composed Image Retrieval with Prescriptive and Proscriptive Constraints,"Composed Image Retrieval (CIR) aims to find a target image that aligns with user intent, expressed through a reference image and a modification text. While Zero-shot CIR (ZS-CIR) methods sidestep the need for labeled training data by leveraging pretrained vision-language models, they often rely on a single fused query that merges all descriptive cues of what the user wants—tending to dilute key information and failing to account for what they wish to avoid. Moreover, current CIR benchmarks assume a single correct target per query, overlooking the ambiguity in modification texts. To address these challenges, we propose Soft Filtering with Textual constraints (SoFT), a training-free, plug-and-play filtering module for ZS-CIR. SoFT leverages multimodal large language models (LLMs) to extract two complementary constraints from the reference-modification pair: prescriptive (must-have) and proscriptive (must-avoid) constraints. These serve as semantic filters that reward or penalize candidate images to re-rank results, without modifying the base retrieval model or adding supervision. In addition, we construct a two-stage dataset pipeline that refines CIR benchmarks. We first identify multiple plausible targets per query to construct multi-target triplets, capturing the open-ended nature of user intent. Then guide multimodal LLMs to rewrite the modification text to focus on one target, while referencing contrastive distractors to ensure precision. This enables more comprehensive and reliable evaluation under varying ambiguity levels. Applied on top of CIReVL—a ZS-CIR retriever—SoFT raises $R@5$ to 65.25 on CIRR (+12.94), $mAP@50$ to 27.93 on CIRCO (+6.13), and $R@50$ to 58.44 on FashionIQ (+4.59), demonstrating broad effectiveness.","Youjin Jung, Seongwoo Cho, Hyun-seok Min, Sungchul Choi","wjd1dbwls@pukyong.ac.kr, jsw6872@pukyong.ac.kr, min6284@gmail.com, sc82.choi@pknu.ac.kr",,,
AAAI26_W41_21,Assessment of RAG and Fine-Tuning for Industrial Question-Answering-Applications,"Large Language Models (LLMs) are increasingly employed in enterprise question-answering (QA) systems, requiring adaptation to domain-specific knowledge. Among the most prevalent methods for incorporating such knowledge are Retrieval-Augmented Generation (RAG) and fine-tuning (FT). Yet, from a cost–accuracy trade-off perspective, it remains unclear which approach best suits industry scenarios. This study investigates the impact of RAG and FT across two closed, industry-specific datasets, evaluating answer quality and operational costs.We extend the Cost-of-Pass framework proposed by Erol et al. (2025) to jointly assess output quality, generation cost, and user interaction cost. Our findings reveal that while premium models perform best out of the box, open-source models can achieve comparable quality when enhanced with RAG. Overall, RAG emerges as the most effective and cost-efficient adaptation method for closed and open source.","Jakob Sturm, Josef Pichlmeier, Christian Bernhard, Maka Karalashvili, Johannes Klepsch, Georg Groh, Andre Luckow","ga46fax@mytum.de, josef.pichlmeier@ifi.lmu.de, christianbernhard089@gmail.com, maka.karalashvili@bmw.de, johannes.klepsch@bmw.de, grohg@in.tum.de, andre.luckow@gmail.com",,,
AAAI26_W41_22,Structured Traversal of Knowledge Graphs for Multi-hop Question Answering,"Multi-hop question-answering tasks pose a significant challenge for retrieval-augmented generation (RAG) models, which must effectively retrieve, infer, and reason across multiple documents to provide accurate answers. Despite the support of RAG, large language models (LLMs) often struggle to consistently retrieve the most relevant information from large collections of documents. To address this issue, we propose a novel approach, BrowseNet involving query-specific traversal within a knowledge graph (KG) for multi-hop information retrieval. This method encodes unstructured text data into a KG, where nodes represent document chunks and edges capture lexical relationships between these chunks. By dynamically traversing the knowledge graph based on the type of multi-hop query, the proposed method enhances retrieval performance by leveraging intrinsic network parameters. We evaluate this method against RAG baselines on publicly available multi-hop query datasets. Experimental results demonstrate that BrowseNet establishes a new state-of-the-art retrieval performance in multi-hop retrieval.","PAVAN KUMAR S, Kiran Kumar Nakka, C Vamshi Krishna Reddy, Divyateja Pasupuleti, Prakhar Agarwal, Harpinder Jot Singh, Anshu Avinash, Nirav Pravinbhai Bhatt","spavaniitm@smail.iitm.ac.in, me20b117@smail.iitm.ac.in, ch20b112@smail.iitm.ac.in, divyateja2004@gmail.com, prakhar.agarwal@devrev.ai, f20170057p@alumni.bits-pilani.ac.in, anshu.avinash@devrev.ai, niravbhatt@smail.iitm.ac.in",,,
AAAI26_W41_23,RAG-MIXER: Query-Aware Selection of Modality and Retrieval Architecture,"Retrieval-Augmented Generation (RAG) systems have demonstrated powerful capabilities. However, their reliance on fixed-pipeline designs, where both the input modality (text or multimodal) and retrieval architecture (dense or late interaction) are static, creates a critical performance bottleneck. High-accuracy pipelines, such as multimodal late-interaction models, offer superior performance on complex queries but introduce significant latency. In contrast, faster, simpler pipelines fail on queries requiring deep visual or semantic understanding. To address this, we propose RAG-MIXER, an intelligent routing framework that dynamically selects the optimal pipeline on a per-query basis. We frame this as a supervised learning problem, where a lightweight classification model is trained to predict the best-suited pipeline option by optimizing a reward function that jointly considers retrieval accuracy and latency. We conduct a rigorous evaluation across eleven diverse retrieval benchmarks, spanning scientific articles, financial reports, and encyclopedic documents, using frontier models for each pipeline configuration. The results confirm that RAG-MIXER achieves superior retrieval accuracy over any single fixed pipeline, while simultaneously reducing average latency by approximately over 37\% compared to exclusively using the most powerful model. Our work demonstrates that an adaptive, query-aware approach is essential for building next-generation RAG systems that are both highly effective and computationally efficient.","Emre Kuru, Mehmet Onur Keskin","emre.kuru@ozu.edu.tr, onur.keskin@ozu.edu.tr",,,
AAAI26_W41_24,CoQuIR: A Benchmark for Code Quality-Aware Information Retrieval,"Code retrieval is vital to modern software engineering as it boosts reuse and speeds up debugging. However, current benchmarks primarily emphasize functional relevance while neglecting code quality. To address this gap, we introduce CoQuIR, the first large-scale, multilingual benchmark specifically designed to evaluate quality-aware code retrieval across four critical dimensions: correctness, efficiency, security, and maintainability. CoQuIR includes fine-grained quality annotations over 42,725 queries and 134,907 code snippets in 11 programming languages. Evaluating 23 retrievers (both open-source and proprietary) reveals that even state-of-the-art models frequently fail to distinguish between buggy or insecure code and robust counterparts. We further investigate methods for explicitly training retrievers to recognize code quality, demonstrating that quality-aware metrics can be improved without loss of semantic relevance; downstream code generation benefits from these gains. CoQuIR underscores the importance of embedding quality signals into retrieval systems as a crucial component for more trustworthy developer tools.","Jiahui Geng, Fengyu Cai, Shaobo Cui, Qing Li, Liangwei Chen, Chenyang Lyu, Haonan Li, Derui Zhu, Alexander Pretschner, Heinz Koeppl, Fakhri Karray","jangandalf365@gmail.com, fengyu.cai@tu-darmstadt.de, cuishaobo16@outlook.com, qing.li@mbzuai.ac.ae, chenliangwei1997@gmail.com, lyuchenyang.dcu@gmail.com, haonan.li@mbzuai.ac.ae, derui.zhu@tum.de, alexander.pretschner@tum.de, heinz.koeppl@bcs.tu-darmstadt.de, fakhri.karray@mbzuai.ac.ae",,,
AAAI26_W41_25,COMPASS: Context-Modulated PID Attention Steering System for Hallucination Mitigation,"Large language models (LLMs) often produce fluent but factually incorrect statements, even when relevant evidence is available, due to misallocation of attention between contextual inputs and parametric knowledge. Ensuring that models actively reason over context and retrieve relevant information is critical for trustworthy and interpretable AI. We introduce COMPASS (Context-Modulated PID Attention Steering System), a lightweight, interpretable framework that dynamically steers attention to retrieved context during generation. Using the Context Reliance Score (CRS), COMPASS identifies which attention heads are underutilizing context, and a PID controller adjusts them in real time to improve evidence grounding and factual consistency. This mechanism enables the model to demonstrate advanced reasoning by actively returning to context and retrieving supporting information when needed, without retraining or multi-pass decoding. Across benchmarks including HotpotQA, XSum, HaluEval, and RAGTruth, COMPASS reduces hallucinations by 2.8–5.8% absolute while revealing how attention heads contribute to context-aligned reasoning. These results show that feedback-driven, interpretable control can enhance reasoning, retrieval, and evidence-based generation in LLMs.","Snigdha Pandya, Rohan Nagale, Kenji Sahay, Anna Lin, Shikhar Shiromani, Kevin Zhu, Sunishchal Dev","snigdhapandya@gmail.com, rohangpu@gmail.com, kenjisahay@gmail.com, annalin2121@gmail.com, rbk.shikhar@gmail.com, kevin@algoverseairesearch.org, sunishchaldev@gmail.com",,,
AAAI26_W41_26,Bridging Symbolic and Neural Reasoning: Ontology-Integrated LLMs for Domain-Grounded QA,"This work presents an ontology-integrated large language model (LLM) framework for chemical engineering that unites structured domain knowledge with generative reasoning. The proposed pipeline aligns model training and inference with the COPE ontology through a sequence of data acquisition, semantic preprocessing, information extraction, and ontology mapping steps, producing templated question-answer pairs that guide fine-tuning. A control-focused decoding stage and citation gate enforce syntactic and factual grounding by constraining outputs to ontology-linked terms, while evaluation metrics quantify both linguistic quality and ontological accuracy. Feedback and future extensions, including semantic retrieval and iterative validation, further enhance the system’s interpretability and reliability. This integration of symbolic structure and neural generation provides a transparent, auditable approach for applying LLMs to process control, safety analysis, and other critical engineering contexts.",Crystal Su,ys3791@columbia.edu,,,
AAAI26_W41_27,Fishing for Answers: Exploring One-shot vs. Iterative Retrieval Strategies for Retrieval Augmented Generation,"Retrieval-Augmented Generation (RAG) based on Large Language Models (LLMs) is a powerful solution to understand and query the industry's closed-source documents. However, basic RAG often struggles with complex QA tasks in legal and regulatory domains, particularly when dealing with numerous government documents. The traditional top-$k$ strategy frequently misses golden chunks, leading to incomplete or inaccurate answers. To address these retrieval bottlenecks, we explore two strategies to improve evidence coverage and answer quality. The first is a One-SHOT retrieval method that adaptively selects chunks based on a token budget, allowing as much relevant content as possible to be included within the LLM's context window.Additionally, we design modules to further filter and refine the chunks. The second is an iterative retrieval strategy built on a Reasoning Agentic RAG framework, where a reasoning LLM dynamically issues search queries, evaluates retrieved results, and progressively refines the context over multiple turns. We identify query drift and retrieval laziness issues and further design two modules to tackle them. Through extensive experiments on a dataset of government documents, we aim to offer practical insights and guidance for real-world applications in legal and regulatory domains. The code and dataset will be released upon acceptance.","Huifeng Lin, sugang, Jintao Liang, You Wu, Rui Zhao, Ziyue Li","linhuifeng@sensetime.com, sugang@sensetime.com, ljt2021@bupt.edu.cn, wuyouscut@gmail.com, zhaorui@sensetime.com, zlibn@connect.ust.hk",,,
AAAI26_W41_28,Towards Better Search with Domain-Aware Text Embeddings for C2C Marketplaces,"Consumer-to-consumer (C2C) marketplaces pose distinct retrieval challenges: short, ambiguous queries; noisy, user- generated listings; and strict production constraints. This pa- per reports our experiment to build a domain-aware Japanese text-embedding approach to improve the quality of search at Mercari, Japan’s largest C2C marketplace. We experimented with fine-tuning on purchase-driven query–title pairs, using role-specific prefixes to model query–item asymmetry. To meet production constraints, we apply Matryoshka Representation Learning to obtain compact, truncation-robust embed- dings. Offline evaluation on historical search logs shows consistent gains over a strong generic encoder, with particularly large improvements when replacing PCA compression with Matryoshka truncation. A manual assessment further high- lights better handling of proper nouns, marketplace-specific semantics, and term-importance alignment. Additionally, an initial online A/B test demonstrates statistically significant improvements in revenue per user and search-flow efficiency, with transaction frequency maintained. Results show that domain-aware embeddings improve relevance and efficiency at scale and form a practical foundation for richer LLM-era search experiences.","Andre Rusli, Miao Cao, Shoma Ishimoto, Sho Akiyama, Max Frenzel","andre.rusli@mercari.com, miao@mercari.com, s-a-ishimoto@mercari.com, s-akiyama@mercari.com, m-frenzel@mercari.com",,,
AAAI26_W41_29,BLUFF-1000: Measuring Uncertainty Expression in RAG,"Retrieval-augmented generation (RAG) systems often fail to adequately modulate their linguistic certainty when evidence deteriorates. This gap in how models respond to imperfect retrieval is critical for the safety and reliability of a real-world RAG system. To address this gap, we propose BLUFF-1000, a benchmark systematically designed to evaluate how large language models (LLMs) manage linguistic confidence under conflicting evidence to simulate poor retrieval. We created a dataset, introduced two novel metrics, and calculated comprehensive metrics to quantify faithfulness, factuality, linguistic uncertainty, and calibration. Finally, we tested generation components of RAG systems with controlled experiments on seven LLMs using the benchmark, measuring their awareness of uncertainty and general performance. While not definitive, our observations reveal initial indications of a misalignment between uncertainty and source quality across seven state-of-the-art RAG systems, underscoring the value of continued benchmarking in this space. We recommend that future RAG systems refine uncertainty-aware methods to convey confidence throughout the system transparently.","Ron Zharzhavsky, Emma Wong, Daniel Ketema","rzharzhavsky@gmail.com, emmawong565@gmail.com, dketema34@gmail.com",,,
AAAI26_W41_30,RAGPart & RAGMask: Retrieval-Stage Defenses Against Corpus Poisoning in Retrieval-Augmented Generation,"Retrieval-Augmented Generation (RAG) has emerged as a promising paradigm to enhance large language models (LLMs) with external knowledge, reducing hallucinations and compensating for outdated information. However, recent studies have exposed a critical vulnerability in RAG pipelines—corpus poisoning—where adversaries inject malicious documents into the retrieval corpus to manipulate model outputs. In this work, we propose two complementary retrieval-stage defenses: RAGPart and RAGMark. Our defenses operate directly on the retriever, making them computationally lightweight and requiring no modification to the generation model. RAGPart leverages the inherent training dynamics of dense retrievers, exploiting document partitioning to mitigate the effect of poisoned points. In contrast, RAGMask identifies suspicious tokens based on significant similarity shifts under targeted token masking. Across two benchmarks, four poisoning strategies, and four state-of-the-art retrievers, our defenses consistently reduce attack success rates while preserving utility under benign conditions. We further introduce an interpretable attack to stress-test our defenses. Our findings highlight the potential and limitations of retrieval-stage defenses, providing practical insights for robust RAG deployments.","Pankayaraj Pathmanathan, Michael-Andrei Panaitescu-Liess, Cho-Yu Jason Chiang, Furong Huang","p.pankayaraj@gmail.com, mpanaite@umd.edu, jchiang@peratonlabs.com, furongh@umd.edu",,,
AAAI26_W41_31,Proactive Interference Reveals Working Memory Limits in LLMs Beyond Context Length,"Information retrieval in Large Language Models (LLMs) is increasingly recognized as intertwined with generation capabilities rather than mere lookup. While longer contexts are often assumed to improve retrieval, the effects of intra-context interference remain understudied. To address this, we adapt the proactive interference (PI) paradigm from cognitive science, where earlier information disrupts recall of newer updates. In humans, susceptibility to such interference is inversely linked to working memory capacity. We introduce PI-LLM, an evaluation that sequentially streams co-referenced key–value updates and queries only the final values. Although these final values are clearly positioned just before the query, LLM retrieval accuracy declines log-linearly toward zero as co-referenced interference accumulates; errors arise from retrieving previously overwritten values. Attempts to mitigate interference via prompt engineering (e.g., instructing models to ignore earlier input) yield limited success. These findings reveal a fundamental constraint on LLMs’ ability to disentangle interference and flexibly manipulate information, suggesting a working memory bottleneck beyond mere context access. We expose a “know, cannot do” failure. Coreference-only needles expose a plan–execute disjunction: LLMs form the correct last-value retrieval plan but fail to carry it out, with execution reliability declining systematically with task complexity. This test advances Needle-in-a-Haystack/MRCR paradigms by eliminating the haystack altogether: By isolating and varying the number of co-referenced “needles,” it directly quantifies interference, revealing a robust log-linear decline in retrieval as interference grows across SOTA models. This calls for approaches that strengthen models’ ability to suppress co-referenced information during retrieval. Code and data are publicly available.","Chupei Wang, Jiaqiu Vince Sun","cw4bb@virginia.edu, js11247@nyu.edu",,,
AAAI26_W41_32,Video-Text Temporal Localization via Multi-Scale Convolution and Dynamic Routing,"Video–text temporal localization requires precise alignment between natural language queries and corresponding video segments, a fundamental challenge in multimodal understanding. We present a novel framework that addresses two critical limitations of existing methods: inadequate modeling of hierarchical temporal structure and inability to handle complex many-to-many correspondences between modalities. Our approach introduces a multi-scale temporal convolutional encoder that captures motion patterns across different temporal granularities—from instantaneous frame transitions to extended action sequences. We further propose a capsule-based dynamic routing mechanism that iteratively refines segment–query associations through structured agreement updates, enabling flexible modeling of non-monotonic alignments. These components are unified through a multi-task learning objective that jointly optimizes temporal boundary regression, cross-modal semantic alignment, and capsule diversity. Extensive experiments on ActivityNet Captions demonstrate significant improvements, achieving 42.9% Recall@0.5 and 41.1% mean IoU, surpassing strong transformer-based baselines while maintaining computational efficiency. Our results validate that combining hierarchical temporal modeling with structured semantic routing provides an effective solution for fine-grained video–language understanding.","Gengtian Shi, Jinze Yu, Chenhao Wu, EIJI FUKUZAWA, Shaofei WANG, Junjie Tang, Hiroshi ONODA, Jiang Liu","shigengtian@akane.waseda.jp, jinze.yu.kp@gmail.com, wuchenhao@toki.waseda.jp, xurong@fuji.waseda.jp, wangshaofei@akane.waseda.jp, junjtang@amazon.com, onoda@waseda.jp, jiang@waseda.jp",,,
AAAI26_W41_33,Revela: Dense Retriever Learning via Language Modeling,"Dense retrievers play a vital role in accessing external and specialized knowledge to augment language models (LMs). Training dense retrievers typically requires annotated query-document pairs, which are costly to create and scarce in specialized domains (e.g., code) or in complex settings (e.g., requiring reasoning). These practical challenges have sparked growing interest in self-supervised retriever learning. Since LMs are trained to capture token-level dependencies through a self-supervised learning objective (i.e., next token prediction), we can analogously cast retrieval as learning dependencies among chunks of tokens. This analogy naturally leads to the question: How can we adapt self‑supervised learning objectives in the spirit of language modeling to train retrievers? To answer this question, we introduce Revela, a unified and scalable training framework for self-supervised retriever learning via language modeling. Revela models semantic dependencies among documents by conditioning next token prediction on local and cross-document context through an in-batch attention mechanism. This attention is weighted by retriever-computed similarity scores, enabling the retriever to be optimized as part of language modeling. We evaluate Revela on domain-specific (CoIR), reasoning-intensive (BRIGHT), and general-domain (BEIR) benchmarks across various retriever backbones. Without annotated or synthetic query-document pairs, Revela surpasses larger supervised models and proprietary APIs on CoIR and matches them on BRIGHT. It achieves BEIR's unsupervised SoTA with ~ 1000x less training data and 10x less compute. Performance increases with batch size and model size, highlighting Revela's scalability and its promise for self‑supervised retriever learning.","Fengyu Cai, Tong Chen, Xinran Zhao, Sihao Chen, Hongming Zhang, Tongshuang Wu, Iryna Gurevych, Heinz Koeppl","fengyu.cai@tu-darmstadt.de, chentong@cs.washington.edu, xinranz3@andrew.cmu.edu, sihaochen@microsoft.com, hzhangal@cse.ust.hk, sherryw@cs.cmu.edu, iryna.gurevych@tu-darmstadt.de, heinz.koeppl@bcs.tu-darmstadt.de",,,
AAAI26_W41_34,Dynamic Tool Dependency Retrieval for Efficient Function Calling,"Function calling agents powered by Large Language Models (LLMs) select external tools to automate complex tasks. On-device agents typically use a retrieval module to select relevant tools, improving performance and reducing context length. However, existing retrieval methods rely on static and limited inputs, failing to capture multi-step tool dependencies and evolving task context. This limitation often introduces irrelevant tools that mislead the agent, degrading efficiency and accuracy. We propose Dynamic Tool Dependency Retrieval (DTDR), a lightweight retrieval method that conditions on both the initial query and the evolving execution context. DTDR models tool dependencies from function calling demonstrations, enabling adaptive retrieval as plans unfold. We benchmark DTDR against state-of-the-art retrieval methods across multiple datasets and LLM backbones, evaluating retrieval precision, downstream task accuracy, and computational efficiency. Additionally, we explore strategies to integrate retrieved tools into prompts. Our results show that dynamic tool retrieval improves function calling success rates between 23% and 104% compared to state-of-the-art static retrievers.","Bhrij Patel, Davide Belli, Amir Jalalirad, Maximilian Arnold, Aleksandr Ermolov, Bence Major","bhrijp11@gmail.com, davidebelli95@gmail.com, ajalalir@qti.qualcomm.com, marnold@qti.qualcomm.com, alex@ermolov.space, bence@qti.qualcomm.com",,,
AAAI26_W41_35,History-Aware Retrieval via Knowledge Unit Graph for Multi-Step Reasoning,"Retrieval-augmented generation (RAG) has emerged as a promising paradigm for mitigating hallucinations in large language models (LLMs). In complex multi-step reasoning scenarios, multi-step RAG approaches, which iteratively generate search queries and retrieve evidence during reasoning, have shown strong potential by leveraging the reasoning capabilities of LLMs. However, existing methods often rely on semantic-based chunk retrieval using only the query from the current reasoning step, ignoring the contextual cues accumulated across previous reasoning steps. This limitation frequently leads to suboptimal retrieval results and impairs downstream reasoning. Furthermore, encoding entire text chunks semantically can obscure fine-grained or diverse knowledge therein, thus restricting the recall of relevant chunks. To overcome these challenges, we extract more pure and independent propositions from text, termed knowledge units. Building upon this foundation, we propose HARK, a novel $\underline{\textbf{H}}$istory-$\underline{\textbf{A}}$ware $\underline{\textbf{R}}$etrieval framework built upon a $\underline{\textbf{K}}$nowledge unit graph. Specifically, HARK extracts salient entities, relation quadruples, and knowledge units from candidate chunks, generates potentially related queries, and organizes all these components into a heterogeneous knowledge unit graph, thereby capturing rich interconnections among them. When retrieving in multi-step reasoning, HARK leverages both the accumulated reasoning context and the graph structure to promote more comprehensive retrieval of relevant chunks. Experiments on several multi-hop question answering (QA) benchmarks demonstrate that HARK effectively enhances the retrieval accuracy and final answer quality within existing multi-step RAG approaches.","Xinyi Wang, Lin Yuan, peilong zhao, Mengshu Sun, Lei Liang","xywang.nju@gmail.com, huiwai.yl@antgroup.com, peilong.zpl@antgroup.com, mengshu.sms@antgroup.com, leywar.liang@antgroup.com",,,
AAAI26_W41_36,ViG-LLM: Enhancing Visual Grounding Capabilities in Closed-Box LLMs for Document Information Extraction without OCR Dependencies,"Large Language Models (LLMs) have shown remarkable capabilities in document processing, but their inability to provide visual grounding without OCR dependencies poses significant challenges in business-critical applications. Current solutions either require model fine-tuning or rely on external OCR services, introducing additional costs, latency, and limitations in handling derived information. This paper presents ViG-LLM, a novel framework that enables closed-box LLMs to generate localization information through a multi-agent system combining U-Net-based layout deconstruction with viewport identification tasks. Evaluated on the FATURA and CORD dataset, our framework achieves perfect accuracy over spatial reasoning tuned LLM like Amazon Nova Pro, while demonstrating superior template-specific consistency. The framework maintains robust performance across LLM architectures while reducing operational costs by 60% compared to Textract-based solutions. In real-world document processing applications, the framework helps retain the high reasoning capabilities of the system in document information extraction tasks while improving explainability, reliability and human interaction for information verification. Through human-in-the-loop learning and closed-box prompt alignment techniques, ViG-LLM provides a robust, adaptable solution for visual grounding tasks in document processing workflows.",Sudhanshu Bhoi,sudhbee@amazon.com,,,
AAAI26_W41_37,From Search to Reasoning: A Five-Level RAG Capability Framework for Enterprise Data,"Retrieval-Augmented Generation (RAG) has emerged as the standard paradigm for answering questions on enterprise data. Traditionally, RAG has centered on text-based semantic search and re-ranking. However, this approach falls short when dealing with questions beyond data summarization or non-text data. This has led to various attempts to supplement RAG to bridge the gap between RAG, the implementation paradigm, and the question answering problem that enterprise users expect it to solve. Given that contemporary RAG is a collection of techniques rather than a defined implementation, discussion of RAG and related question-answering systems benefits from a problem-oriented understanding. We propose a new classification framework (L1–L5) to categorize systems based on data modalities and task complexity of the underlying question answering problems: L1 (Surface Knowledge of Unstructured Data) through L4 (Reflective and Reasoned Knowledge) and the aspirational L5 (General Intelligence). We also introduce benchmarks aligned with these levels and evaluate four state-of-the-art platforms: LangChain, Azure AI Search, OpenAI and Corvic AI. Our experiments highlight the value of multi-space retrieval and dynamic orchestration for enabling L1–L4 capabilities. We empirically validate our findings using diverse datasets indicative of enterprise use cases.","Gurbinder Gill, Ritvik Gupta, Denis Lusson, Anand Chandrashekar, Donald Nguyen","gill@corvic.ai, ritvikgu@cs.cmu.edu, denis@corvic.ai, anand@corvic.ai, ddn@corvic.ai",,,